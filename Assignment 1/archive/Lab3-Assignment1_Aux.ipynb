{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2735c935",
   "metadata": {
    "id": "2735c935"
   },
   "source": [
    "<h1><center> Assignment 1: EDA United Nations General Debate Corpus  </center></h1>\n",
    "\n",
    "We are now going to give the first steps into exploring the United Nations General Debate Corpus. <span style=\"color:red\">This dataset will be used in Group Assignment I (due date, Monday 29 September, 23:59)</span>. It is expected that you will pose a question about the dataset, explore it, and combine it with other datasets (e.g., the Happiness Report 2023 that we've been using, or the International Trade Dataset, or any other of your choice).\n",
    "\n",
    "We will use the *the UN General Debate Corpus (UNGDC)*, which introduces the corpus of texts of UN General Debate statements from 1946 (Session 1) to 2023 (Session 78). More info [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0TJX8Y). Make sure to download the file <code>UNGDC_1946-2024.tar.gz</code> and extract the folder <code>TXT/</code> to the same directory as the current Jupyter notebook.\n",
    "\n",
    "Notice that the 80th session of the UN General Assembly - where the 2025 debates will happen - will occur in September 2024, precisely during the time you'll be working in Assignment 1. More info [here](https://www.un.org/en/ga/).\n",
    "\n",
    "You might find useful to have a dataset with the full name and 3-code description of countries. You can find that data [here](https://unstats.un.org/unsd/methodology/m49/overview/). Download the corresponding CSV file (named 'UNSD — Methodology.csv') and place it into the same folder as this notebook.\n",
    "\n",
    "We will start by loading the speeches text to a table:"
   ]
  },
  {
   "cell_type": "code",
   "id": "cd0b3ce3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "error",
     "timestamp": 1694776892897,
     "user": {
      "displayName": "York Pudds (Yorkie)",
      "userId": "15802187727162843994"
     },
     "user_tz": -120
    },
    "id": "cd0b3ce3",
    "outputId": "da1cfd65-38c1-4971-dd05-c2a17ce22140",
    "ExecuteTime": {
     "end_time": "2025-09-22T10:11:15.317559Z",
     "start_time": "2025-09-22T10:11:14.174517Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sessions = np.arange(25, 76)\n",
    "data=[]\n",
    "\n",
    "for session in sessions:\n",
    "    directory = \"./Data/TXT/Session \"+str(session)+\" - \"+str(1945+session)\n",
    "    # directory = f\"./TXT/Session {session} - {1945+session}\"\n",
    "    for filename in os.listdir(directory):\n",
    "        # f = open(os.path.join(directory, filename))\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "            if filename[0]==\".\": #ignore hidden files\n",
    "                continue\n",
    "            splt = filename.split(\"_\")\n",
    "            data.append([session, 1945+session, splt[0], f.read()])\n",
    "\n",
    "df_speech = pd.DataFrame(data, columns=['Session','Year','ISO-alpha3 Code','Speech'])\n",
    "\n",
    "df_speech.tail()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Session  Year ISO-alpha3 Code  \\\n",
       "8476       75  2020             HRV   \n",
       "8477       75  2020             GAB   \n",
       "8478       75  2020             MCO   \n",
       "8479       75  2020             AND   \n",
       "8480       75  2020             BHR   \n",
       "\n",
       "                                                 Speech  \n",
       "8476  Mr President, Excellencies\\nAll protocol obser...  \n",
       "8477  Mr. President, Majesties,\\nLadies and Gentleme...  \n",
       "8478  Mr. President of the General Assembly,\\nMr. Se...  \n",
       "8479  Mr. President,\\nMr. Secretary General,\\nYour E...  \n",
       "8480  In the name of Allah, the most gracious, the m...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Session</th>\n",
       "      <th>Year</th>\n",
       "      <th>ISO-alpha3 Code</th>\n",
       "      <th>Speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>75</td>\n",
       "      <td>2020</td>\n",
       "      <td>HRV</td>\n",
       "      <td>Mr President, Excellencies\\nAll protocol obser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8477</th>\n",
       "      <td>75</td>\n",
       "      <td>2020</td>\n",
       "      <td>GAB</td>\n",
       "      <td>Mr. President, Majesties,\\nLadies and Gentleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8478</th>\n",
       "      <td>75</td>\n",
       "      <td>2020</td>\n",
       "      <td>MCO</td>\n",
       "      <td>Mr. President of the General Assembly,\\nMr. Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8479</th>\n",
       "      <td>75</td>\n",
       "      <td>2020</td>\n",
       "      <td>AND</td>\n",
       "      <td>Mr. President,\\nMr. Secretary General,\\nYour E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8480</th>\n",
       "      <td>75</td>\n",
       "      <td>2020</td>\n",
       "      <td>BHR</td>\n",
       "      <td>In the name of Allah, the most gracious, the m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "072f332c",
   "metadata": {
    "id": "072f332c"
   },
   "source": [
    "Download the 'UNSD — Methodology.csv' ([link](https://unstats.un.org/unsd/methodology/m49/overview/)) file and and try to load it. Please check what is the separator used. Why is that separator used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e54e04",
   "metadata": {
    "id": "60e54e04",
    "outputId": "2dda9a07-4c8b-4e52-e8a0-744f58857013"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcfe8b4d",
   "metadata": {
    "id": "bcfe8b4d"
   },
   "source": [
    "**Q1: Can you create a merged DataFrame? — merge between df_codes and df_speech according to ISO-alpha3 and composed of columns \\[\"Country or Area\", \"Region Name\",\"Sub-region Name\", \"ISO-alpha3 Code\",\"Least Developed Countries (LDC)\", \"Session\", \"Year\", \"Speech\"\\]? It would be convinient to have index as (Year, 'ISO-alpha3 Code')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde2a21b",
   "metadata": {
    "id": "cde2a21b",
    "outputId": "00428c18-fc02-4953-f6d3-d1921abc52d4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbc0358d",
   "metadata": {
    "id": "fbc0358d"
   },
   "source": [
    "We are now going to use NLTK\n",
    "\n",
    "Please run the cell below to import NLTK and download the needed resources. In case of errors, please see those two Github issues: [initial bug report](https://github.com/nltk/nltk/issues/3308) [latest fix attempt](https://github.com/nltk/nltk/issues/3416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d6e45",
   "metadata": {
    "id": "d49d6e45",
    "outputId": "77120fca-6e61-45a6-c938-dc6bbe16bfb9"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822564c9",
   "metadata": {
    "id": "822564c9"
   },
   "source": [
    "Let us now see some examples of word analysis with NLTK:\n",
    "\n",
    "Which were the most frequent words used in the Austrian Speech in 1970?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10182ad0",
   "metadata": {
    "id": "10182ad0",
    "outputId": "d808c3c9-d99d-47f4-cd9b-cadae1d06d7a"
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# load text of Austria in 1970\n",
    "text = df_un_merged.loc[1970,'AUT'][\"Speech\"]\n",
    "\n",
    "# tokenize words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# compute word frequency\n",
    "freq = FreqDist(words)\n",
    "\n",
    "# show 30 most frequent words\n",
    "freq.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b626b302-6072-48d0-8895-9824404b49d1",
   "metadata": {
    "id": "b626b302-6072-48d0-8895-9824404b49d1"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6593e2",
   "metadata": {
    "id": "4a6593e2",
    "outputId": "9acfffe4-4b57-42a2-ee4f-5cd9a278134e"
   },
   "outputs": [],
   "source": [
    "# plot the histogram with the top most used words\n",
    "freq.plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab49af4",
   "metadata": {
    "id": "1ab49af4"
   },
   "source": [
    "Notice that the most frequent words are not that informative about the Austrian speech (the, of, to...). These words are often called *stop-words*. These words are generally filtered out before processing text (natural language). These are actually some of the most common words in any language (articles, prepositions, pronouns, conjunctions, etc) but do not add much information to the text. Let's now use NLTK to filter those words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93adb656",
   "metadata": {
    "id": "93adb656",
    "outputId": "2a61f41a-7ce0-43d8-ca05-f3c3475459dc"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess(words):\n",
    "    sw = stopwords.words(\"english\")\n",
    "    no_sw = []\n",
    "    for w in words:\n",
    "        if (w not in sw):\n",
    "            no_sw.append(w)\n",
    "    return no_sw\n",
    "\n",
    "text = df_un_merged.loc[2002,\"AFG\"][\"Speech\"]\n",
    "\n",
    "words = word_tokenize(text)\n",
    "words = preprocess(words)\n",
    "freq = FreqDist(words)\n",
    "\n",
    "freq.plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5081d6eb",
   "metadata": {
    "id": "5081d6eb"
   },
   "source": [
    "**Q2: Can you change the method preprocess to put all words in lower case, remove punctuation and remove non-informative words (e.g., United Nations)?**\n",
    "\n",
    "Tip: the method isalpha() might be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc88891e",
   "metadata": {
    "id": "cc88891e",
    "outputId": "9f4f822f-b5ea-4570-8194-c1b7abbd2b15"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44ba5961",
   "metadata": {
    "id": "44ba5961"
   },
   "source": [
    "A regular expression is a sequence of characters that specifies a pattern. Usually, such patterns are used by to find, match, replace sub-strings within a document. Regular expressions have a particular syntax and are often useful to clean and pre-process textual data. Here one example where the regular expression 'afg.\\*' is used to match any word that starts with afg and is followed by any character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd5015",
   "metadata": {
    "id": "73cd5015",
    "outputId": "9b44ad57-28b1-4fd2-caac-09fe2c0c9c98"
   },
   "outputs": [],
   "source": [
    "# Regular expression example\n",
    "s = set({})\n",
    "import re\n",
    "for w in words:\n",
    "    if re.match('afg.*n$', w):\n",
    "        s.add(w)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1cdfdc",
   "metadata": {
    "id": "bf1cdfdc"
   },
   "source": [
    "Another useful usage of NLTK is performing sentiment analysis.\n",
    "\n",
    "Sentiment analysis can be seen as the process of automatically classifying text into positive or negative sentiment categories. With NLTK, you can employ these algorithms without effort. This was also called opinion mining.\n",
    "\n",
    "In the political field, sentiment analysis is used to keep track of political view, to detect consistency and inconsistency between statements and actions at the government level or to derive the opinion or attitude of a speaker.\n",
    "\n",
    "NLTK implements VADER (Valence Aware Dictionary and sEntiment Reasoner), which is a lexicon and rule-based sentiment analysis. VADER uses a list of lexical features (e.g., words) which are generally labeled according to their semantic orientation as either positive or negative. VADER not only tells about the Positivity and Negativity score but also tells us about how positive or negative a sentiment is.\n",
    "\n",
    "NLTK implements VADER through the module SentimentIntensityAnalyzer. Below an example of application (with natural limitations as VADER is specifically attuned to sentiments expressed in **social media**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293744e6",
   "metadata": {
    "id": "293744e6",
    "outputId": "e204f46d-02aa-46b7-9aa9-59f0e6195aca"
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "vecUSA = [sia.polarity_scores(df_un_merged.loc(axis=0)[i,\"USA\"][\"Speech\"])['pos'] for i in np.arange(1971, 2021)]\n",
    "vecRUS = [sia.polarity_scores(df_un_merged.loc(axis=0)[i,\"RUS\"][\"Speech\"])['pos'] for i in np.arange(1971, 2021)]\n",
    "vecCHN = [sia.polarity_scores(df_un_merged.loc(axis=0)[i,\"CHN\"][\"Speech\"])['pos'] for i in np.arange(1971, 2021)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(1971, 2021), vecUSA, label='USA')\n",
    "ax.plot(np.arange(1971, 2021), vecRUS, label='RUS')\n",
    "ax.plot(np.arange(1971, 2021), vecCHN, label='CHN')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Positive Sentiment Score')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
